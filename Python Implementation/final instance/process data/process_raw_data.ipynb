{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd00bdb1145a6394107ddd55d824c4d0e411e79cd19a6286b1018600d724ae6ee81",
   "display_name": "Python 3.8.8 64-bit ('dynocog': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Process Raw Data\n",
    "\n",
    "\n",
    "This Notebook provides the code to process the raw _.txt_ files collected by the Psytoolkit Experiments.\n",
    "\n",
    "Experiments:\n",
    " - Navon\n",
    " - Fitts\n",
    " - N-Back\n",
    " - WCST\n",
    " - Corsi Block Span\n",
    "\n",
    "\n",
    "#### Function\n",
    "Extract raw data from _.txt_ files & store them in a programable datastructure (pandas dataframes)\n",
    "\n",
    "#### Input\n",
    "- file path: raw _.txt_ data\n",
    "- file path: write _pandas dataframes_\n",
    "\n",
    "#### Output\n",
    "- stored, structured dataframes suitable for statistical computation\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "```\n",
    "Zach Wolpe\n",
    "zachcolinwolpe@gmail.com\n",
    "27 May 2021\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_processing:\n",
    "    \"\"\"\n",
    "    Input:  path to data \n",
    "    Return: tools too write pandas dataframes of data to a specified location\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_data):\n",
    "        self.path          = path_to_data\n",
    "        self.metadata      = None\n",
    "        self.mapping       = pd.read_csv(self.path + '/data.csv', index_col=False)\n",
    "        self.data_times    = pd.read_csv(self.path + '/data_times.csv', index_col=False)\n",
    "        self.participants  = self.mapping['participant'].tolist()\n",
    "        self.parti_code    = self.mapping['participant_code:1'].tolist()\n",
    "        self.n             = self.mapping.shape[0]\n",
    "        self.wcst_paths    = [self.path  + wp for wp in self.mapping['wcst_task:1'].tolist()]\n",
    "        self.nback_paths   = [self.path  + wp for wp in self.mapping['n_back_task:1'].tolist()]\n",
    "        self.corsi_paths   = [self.path  + wp for wp in self.mapping['corsi_block_span_task:1'].tolist()]\n",
    "        self.fitts_paths   = [self.path  + wp for wp in self.mapping['fitts_law:1'].tolist()]\n",
    "        self.navon_paths   = [self.path  + wp for wp in self.mapping['navon_task:1'].tolist()]\n",
    "        self.wcst_data     = None\n",
    "        self.nback_data    = None\n",
    "        self.corsi_data    = None\n",
    "        self.fitts_data    = None\n",
    "        self.navon_data    = None\n",
    "    \n",
    "    def cache_metadata(self, metadata_path):\n",
    "        \"\"\"Store any metadata\"\"\"\n",
    "        self.meta_data = pd.read_csv(metadata_path, index_col=False)\n",
    "        print('')\n",
    "        print('     Metadata saved!     ')\n",
    "        print('')\n",
    "\n",
    "    def create_wcst_data(self):\n",
    "        message = \"\"\"\n",
    "\n",
    "        ------------------------------------------------------------------\n",
    "                                WCST data created\n",
    "        ------------------------------------------------------------------\n",
    "\n",
    "        \"\"\"\n",
    "        print(message)\n",
    "        df = pd.DataFrame()\n",
    "        for p in range(self.n):\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            pc = self.parti_code[p]\n",
    "            pt = self.participants[p]\n",
    "\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            f = open(self.wcst_paths[p], 'r')\n",
    "            for l in f.readlines():\n",
    "                st  = l.split(' ')\n",
    "                crd = re.split(r'(\\d+)', st[5]) \n",
    "                dt  = {\n",
    "                    'participant':            pc,\n",
    "                    'participant_code':       pt,\n",
    "                    'card_no':                st[0],\n",
    "                    'correct_card':           st[1],\n",
    "                    'correct_persevering':    st[2],\n",
    "                    'seq_no':                 st[3],\n",
    "                    'rule':                   st[4],\n",
    "                    'card_shape':             crd[0],\n",
    "                    'card_number':            crd[1],\n",
    "                    'card_colour':            crd[2],\n",
    "                    'reaction_time_ms':       st[6],\n",
    "                    'status':                 st[7],\n",
    "                    'card_selected':          st[8],\n",
    "                    'error':                  st[9],\n",
    "                    'perseverance_error':     st[10],\n",
    "                    'not_perseverance_error': st[11].split('\\n')[0],\n",
    "                }\n",
    "                df = df.append(dt, ignore_index=True)[dt.keys()]\n",
    "        f.close()\n",
    "        self.wcst_data = df\n",
    "\n",
    "\n",
    "    def create_navon_data(self):\n",
    "        message = \"\"\"\n",
    "\n",
    "        ------------------------------------------------------------------\n",
    "                                Navon data created\n",
    "        ------------------------------------------------------------------\n",
    "\n",
    "        \"\"\"\n",
    "        print(message)\n",
    "        df = pd.DataFrame()\n",
    "        for p in range(self.n):\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            pc = self.parti_code[p]\n",
    "            pt = self.participants[p]\n",
    "\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            f = open(self.navon_paths[p], 'r')\n",
    "            for l in f.readlines():\n",
    "                st  = l.split(' ')\n",
    "                dt  = {\n",
    "                    'participant':            pc,\n",
    "                    'participant_code':       pt,\n",
    "                    'large_letter':           st[0][0],\n",
    "                    'small_letter':           st[0][0],\n",
    "                    'level_of_target':        st[1],\n",
    "                    'level_of_target_n':      st[2],\n",
    "                    'status':                 st[3],\n",
    "                    'reaction_time_ms':       st[4].split('\\n')[0],\n",
    "                }\n",
    "                df = df.append(dt, ignore_index=True)[dt.keys()]\n",
    "        f.close()\n",
    "        self.navon_data = df\n",
    "\n",
    "\n",
    "    def create_nback_data(self):\n",
    "        message = \"\"\"\n",
    "\n",
    "        ------------------------------------------------------------------\n",
    "                                N back data created\n",
    "        ------------------------------------------------------------------\n",
    "\n",
    "        \"\"\"\n",
    "        print(message)\n",
    "        df = pd.DataFrame()\n",
    "        for p in range(self.n):\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            pc = self.parti_code[p]\n",
    "            pt = self.participants[p]\n",
    "\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            f = open(self.nback_paths[p], 'r')\n",
    "            for l in f.readlines():\n",
    "                st  = l.split(' ')\n",
    "                dt  = {\n",
    "                    'participant':              pc,\n",
    "                    'participant_code':         pt,\n",
    "                    'block_number':             st[0],\n",
    "                    'score':                    st[1],\n",
    "                    'status':                   st[2],\n",
    "                    'miss':                     st[3],\n",
    "                    'false_alarm':              st[4],\n",
    "                    'reaction_time_ms':         st[5],\n",
    "                    'match':                    st[6],\n",
    "                    'stimuli':                  st[7],\n",
    "                    'stimuli_n_1':              st[8],\n",
    "                    'stimuli_n_2':              st[9].split('\\n')[0],\n",
    "                }\n",
    "                df = df.append(dt, ignore_index=True)[dt.keys()]\n",
    "        f.close()\n",
    "        self.nback_data = df\n",
    "\n",
    "\n",
    "    def create_corsi_data(self):\n",
    "        message = \"\"\"\n",
    "\n",
    "        ------------------------------------------------------------------\n",
    "                                Corsi data created\n",
    "        ------------------------------------------------------------------\n",
    "\n",
    "        \"\"\"\n",
    "        print(message)\n",
    "        df = pd.DataFrame()\n",
    "        for p in range(self.n):\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            pc = self.parti_code[p]\n",
    "            pt = self.participants[p]\n",
    "\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            f = open(self.corsi_paths[p], 'r')\n",
    "            for l in f.readlines():\n",
    "                st  = l.split(' ')\n",
    "                dt  = {\n",
    "                    'participant':              pc,\n",
    "                    'participant_code':         pt,\n",
    "                    'highest_span':             st[0],\n",
    "                    'n_items':                  st[1],\n",
    "                    'status':                   st[2].split('\\n')[0],\n",
    "                }\n",
    "                df = df.append(dt, ignore_index=True)[dt.keys()]\n",
    "        f.close()\n",
    "        self.corsi_data = df\n",
    "\n",
    "\n",
    "\n",
    "    def create_fitts_data(self):\n",
    "        message = \"\"\"\n",
    "\n",
    "        ------------------------------------------------------------------\n",
    "                                Fitts data created\n",
    "        ------------------------------------------------------------------\n",
    "\n",
    "        \"\"\"\n",
    "        print(message)\n",
    "        df = pd.DataFrame()\n",
    "        for p in range(self.n):\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            pc = self.parti_code[p]\n",
    "            pt = self.participants[p]\n",
    "\n",
    "            # _____ FOR EACH PARTICIPANT -----x\n",
    "            f = open(self.fitts_paths[p], 'r')\n",
    "            for l in f.readlines():\n",
    "                st  = l.split(' ')\n",
    "                dt  = {\n",
    "                    'participant':              pc,\n",
    "                    'participant_code':         pt,\n",
    "                    'x_loc':                    st[0],\n",
    "                    'y_loc':                    st[1],\n",
    "                    'size':                     st[2],\n",
    "                    'distance':                 st[3],\n",
    "                    'fitts_prediction':         st[4],\n",
    "                    'reaction_time_ms':         st[5],\n",
    "                    'status':                   st[6].split('\\n')[0],\n",
    "                }\n",
    "                df = df.append(dt, ignore_index=True)[dt.keys()]\n",
    "        f.close()\n",
    "        self.fitts_data = df\n",
    "\n",
    "\n",
    "\n",
    "    def convert_data_to_int(self):\n",
    "        \"\"\"Change the schema of the dataframes to include integers\"\"\"\n",
    "        # converter function\n",
    "        def str_to_int(df, columns):\n",
    "            for c in columns: df[c] = df[c].astype(int)\n",
    "            return(df)\n",
    "\n",
    "        # convert schemas\n",
    "        self.fitts_data = str_to_int(self.fitts_data, \n",
    "        ['x_loc', 'y_loc', 'size', 'distance', 'fitts_prediction', 'reaction_time_ms', 'status'])\n",
    "\n",
    "        self.corsi_data = str_to_int(self.corsi_data, ['highest_span', 'n_items', 'status'])\n",
    "\n",
    "        self.nback_data = str_to_int(self.nback_data, \n",
    "        ['block_number', 'score', 'status','miss', 'false_alarm', 'reaction_time_ms', 'match', \n",
    "        'stimuli','stimuli_n_1', 'stimuli_n_2'])\n",
    "\n",
    "        self.wcst_data = str_to_int(self.wcst_data, \n",
    "        ['card_no', 'correct_card', 'correct_persevering', 'seq_no', 'card_number', 'reaction_time_ms', 'status', \n",
    "        'card_selected', 'error', 'perseverance_error', 'not_perseverance_error'])\n",
    "\n",
    "        self.navon_data = str_to_int(self.navon_data, ['level_of_target_n', 'status', 'reaction_time_ms'])\n",
    "        message=\"\"\"\n",
    "        ------------------------------------------------------------------\n",
    "        Schemas Converted!\n",
    "        ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        print(message)\n",
    "\n",
    "\n",
    "    def write_to_pickle(self, path):\n",
    "        \"\"\"Write the data to pickle files\"\"\"\n",
    "        try: os.mkdir(path)\n",
    "        except: None\n",
    "\n",
    "        self.fitts_data.to_pickle(path + 'fitts_data.pkl')\n",
    "        self.wcst_data.to_pickle(path  + 'wcst_data.pkl')\n",
    "        self.nback_data.to_pickle(path + 'nback_data.pkl')\n",
    "        self.corsi_data.to_pickle(path + 'corsi_data.pkl')\n",
    "        self.navon_data.to_pickle(path + 'navon_data.pkl')\n",
    "        message=\"\"\"\n",
    "        ------------------------------------------------------------------\n",
    "        Dataframes successfully written to path {}!\n",
    "        ------------------------------------------------------------------\n",
    "        \"\"\".format(path)\n",
    "        print(message)\n",
    "\n",
    "\n",
    "    def read_from_pickle(self, path):\n",
    "        \"\"\"Read the data to pickle files\"\"\"\n",
    "        self.fitts_data = pd.read_pickle(path + 'fitts_data.pkl')\n",
    "        self.wcst_data  = pd.read_pickle(path + 'wcst_data.pkl')\n",
    "        self.nback_data = pd.read_pickle(path + 'nback_data.pkl')\n",
    "        self.corsi_data = pd.read_pickle(path + 'corsi_data.pkl')\n",
    "        self.navon_data = pd.read_pickle(path + 'navon_data.pkl')\n",
    "        message=\"\"\"\n",
    "        ------------------------------------------------------------------\n",
    "        Dataframes:\n",
    "\n",
    "            - fitts_data\n",
    "            - wcst_data\n",
    "            - nback_data\n",
    "            - corsi_data\n",
    "            - navon_data\n",
    "\n",
    "        Successfully read from path: \\'{}\\'!\n",
    "        ------------------------------------------------------------------\n",
    "        \"\"\".format(path)\n",
    "        print(message)\n",
    "\n",
    "\n",
    "    def write_class_to_pickle(self, path):\n",
    "        \"\"\"serialize object to pickle object\"\"\"\n",
    "\n",
    "        #save it\n",
    "        filename = path + 'batch_processing_object.pkl'\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(bp, file) \n",
    "\n",
    "        # #load it\n",
    "        # with open(filename, 'rb') as file2:\n",
    "        #     bp = pickle.load(file2)\n",
    "        message=\"\"\"\n",
    "        ------------------------------------------------------------------\n",
    "        Object successfully written to path: \\'{}\\'!\n",
    "\n",
    "        To retrieve run:\n",
    "            with open(\\'{}\\', 'rb') as file2:\n",
    "                bp = pickle.load(file2)\n",
    "        ------------------------------------------------------------------\n",
    "        \"\"\".format(filename, filename)\n",
    "        print(message)"
   ]
  },
  {
   "source": [
    "# Instantiate\n",
    "Instantiate the class & provide the file locations. Save the processed object as a _.pkl_ file - compressing the datastructure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-48e516a7b374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data objects/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmeta\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0;34m'../../../Data/mturk/Batch_4453805_batch_results - 6 May 2021.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbp\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mbatch_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-87c93ff59145>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparti_code\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'participant_code:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwcst_paths\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wcst_task:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnback_paths\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_back_task:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorsi_paths\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corsi_block_span_task:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-87c93ff59145>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparti_code\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'participant_code:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwcst_paths\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wcst_task:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnback_paths\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_back_task:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorsi_paths\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corsi_block_span_task:1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "path  = '../../../Data/mturk/data - MTurk- 26 May 2021'\n",
    "path2 = '../data objects/'\n",
    "meta  ='../../../Data/mturk/Batch_4453805_batch_results - 6 May 2021.csv'\n",
    "bp   = batch_processing(path)\n",
    "\n",
    "\n",
    "# bp.create_wcst_data()\n",
    "# bp.create_navon_data()\n",
    "# bp.create_nback_data()\n",
    "# bp.create_corsi_data()\n",
    "# bp.create_fitts_data()\n",
    "# bp.convert_data_to_int()\n",
    "# bp.write_to_pickle(path2)\n",
    "# bp.read_from_pickle(path2)\n",
    "# bp.write_class_to_pickle(path2)\n",
    "\n",
    "\n",
    "        self.mapping       = pd.read_csv(self.path + '/data.csv', index_col=False)\n",
    "        self.data_times    = pd.read_csv(self.path + '/data_times.csv', index_col=False)\n",
    "        self.participants  = self.mapping['participant'].tolist()\n",
    "        self.parti_code    = self.mapping['participant_code:1'].tolist()\n",
    "        self.n             = self.mapping.shape[0]\n",
    "        self.wcst_paths    = [self.path  + wp for wp in self.mapping['wcst_task:1'].tolist()]\n",
    "        self.nback_paths   = [self.path  + wp for wp in self.mapping['n_back_task:1'].tolist()]\n",
    "        self.corsi_paths   = [self.path  + wp for wp in self.mapping['corsi_block_span_task:1'].tolist()]\n",
    "        self.fitts_paths   = [self.path  + wp for wp in self.mapping['fitts_law:1'].tolist()]\n",
    "        self.navon_paths   = [self.path  + wp for wp in self.mapping['navon_task:1'].tolist()]\n",
    "        self.wcst_data     = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null Entries:  101\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(path + '/data.csv', index_col=False)\n",
    "\n",
    "\n",
    "print('Null Entries: ', np.sum([t != t for t in test['wcst_task:1'].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "valid entries:  233\n"
     ]
    }
   ],
   "source": [
    "test.head()\n",
    "print('valid entries: ', test.shape[0] - np.sum([t != t for t in test['wcst_task:1'].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}