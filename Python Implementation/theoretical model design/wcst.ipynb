{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# WCST: Wisconsin Card Sorting Test\n",
    "\n",
    "Theoretical description of the modelling framework: wcst.\n",
    "\n",
    "------------\n",
    "# Action Space: Output Space\n",
    "\n",
    "The $\\mathcal{A}$ action space defines the possible actions (or outputs) the agent can take. There are $3$ possible actions in the task, corresponding to the matching rules:\n",
    "\n",
    "$$: \n",
    "\\begin{equation}\n",
    "    \\mathcal{A} := \n",
    "    \\begin{cases}\n",
    "      c, & \\text{match on colour} \\\\\n",
    "      s, & \\text{match on shape} \\\\\n",
    "      n, & \\text{match on number} \n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "$$\n",
    "\n",
    "Encoded as:\n",
    "\n",
    "$$: \n",
    "\\{c,s,n\\} \\rightarrow \\{-1,0,1\\}\n",
    "$$\n",
    "\n",
    "\n",
    "------------\n",
    "\n",
    "\n",
    "# StateSpace: Input Space\n",
    "\n",
    "\n",
    "We are working with a bandit as we only have $1$ temporal state (there is no dependency across time outside of the current state valuation). We wish to maximise return by selecting the action that yields the highest expected return, that is, our policy $\\pi(\\alpha)$ is to select the action $\\alpha$ with the highest expected reward $Q_t(\\alpha)$ at time $t$:\n",
    "\n",
    "$$\\pi(\\alpha) = \\underset{\\alpha}{argmax} \\; Q_t(\\alpha)$$\n",
    "\n",
    "where the action space is given by the decision rule:\n",
    "\n",
    "$$actions = \\alpha = \\{\"shape\", \"colour\", \"number\" \\}$$\n",
    "\n",
    "----- \n",
    "The simplist calculation of this would be to let the expected return of each action be the sample average return from trying that action. That is to say:\n",
    "\n",
    "$$Q_t(\\alpha) = \\frac{1}{t-1}\\sum_1^{t-1}r_i$$\n",
    "\n",
    "where $r_t$ is the binary reward received after taking an action:\n",
    "\n",
    "$$r_t \\in\\{0,1\\}$$\n",
    "\n",
    "Note: for faster computation & more efficient memory, this average calculation can be written recursively:\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\frac{1}{n}[r_t - Q_{t-1}]$$\n",
    "\n",
    "Two problems arise:\n",
    "- This ignores non-stationarity (the rule changes) and as such more recent feedback must carry more weight.\n",
    "- This ignores the interaction effect of the game, knowledge about one action actually gives great info about the other actions as they are mutually exclusive. Should we encode this? It can be learnt through more aggressive updating.\n",
    "\n",
    "The $\\frac{1}{n}$ term effectively weights all returns $r_i$ equally. Thus this issues can be mitigated by overweighting newer returns & underweighting older returns. This can be achieved by setting this updating parameter to a constant $\\lambda$:\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\lambda[r_t - Q_{t-1}]$$\n",
    "\n",
    "-----\n",
    "\n",
    "Recall that $r_t \\in\\{0,1\\}$, thus:\n",
    "\n",
    "If $r_t = 1$ (correct choice):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= Q_{t-1} + \\lambda[r_t - Q_{t-1}] \\\\\n",
    "    Q_t &= Q_{t-1} + \\lambda[1 - Q_{t-1}] \\\\\n",
    "    Q_t &= \\lambda + (1-\\lambda)Q_{t-1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Or, if $r_t = 0$ (incorrect choice):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= Q_{t-1} + \\lambda[r_t - Q_{t-1}] \\\\\n",
    "    Q_t &= Q_{t-1} + \\lambda[0 - Q_{t-1}] \\\\\n",
    "    Q_t &= (1-\\lambda)Q_{t-1} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It's trivial to see that $\\lambda=1$ (the extreme case) would be choice the desired response:\n",
    "\n",
    "\n",
    "If $r_t = 1$ & $\\lambda=1$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= \\lambda + (1-\\lambda)Q_{t-1} \\\\\n",
    "    Q_t &= 1 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Or, if $r_t = 0$ & $\\lambda=1$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= (1-\\lambda)Q_{t-1} \\\\\n",
    "    Q_t &= 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which is correct as only one choice can be the correct choice. This is the extreme weighting, where we only take the value of the most recent return.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "------------\n",
    "------------\n",
    "------------\n",
    "\n",
    "```\n",
    "author: Zach Wolpe\n",
    "email:  zachcolinwolpe@gmail.com\n",
    "date:   22 June 2021\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define Statespace\n",
    "\n",
    "Define also possible variables in the model, formalising the task as a probabilistic mathematical construct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Depricated\n",
    "\n",
    "# RL WCST Agent 001\n",
    "\n",
    "The first, simplest, RL agent. Showcasing the basic structure.\n",
    "\n",
    "# MÃ¸del Development\n",
    "\n",
    "1. Consider the simplest possible model:\n",
    "    - 1 agent learning the wcst overtime\n",
    "    - no Bayes\n",
    "    - no data\n",
    " \n",
    "2. Fitting to data == infering the learning rate. \n",
    "    - Optimisation techniques? \n",
    "    - Performance criterion?\n",
    "\n",
    "3. Additonal Experiments\n",
    "    - Capture variation **between** individuals\n",
    "    - Model neurocorrelates\n",
    "\n",
    "4. Heirarchical Bayes\n",
    "    - Capture varation **across** individuals\n",
    "\n",
    "## Optimality\n",
    "\n",
    "The optimal learning behaviour would be too simply choose the last correct action. This would equate to an update equation that totally saturates previous information & essentially only considers the most recent but of information.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " # Build RL Environment\n",
    "\n",
    "Now we build the RL environment that can be used to train the parameters.\n",
    "\n",
    "In our model the RL environment reflects WCST.\n",
    "\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "Dictionaries are far faster than lists in Python, as such we structure each sample as a dictionary. Each environmental sample has two components:\n",
    "- Current card: the card drawn\n",
    "- Target card: the correct matching option\n",
    "- Rule: the matching rule\n",
    "\n",
    "Each of these are captured in a dictionary & each observation put into a list object called 'item'. Each item is then added to a list (giving us a list of observations) which is later conververted into a enumerable object.\n",
    "\n",
    "Return: a list of observations & outputs representing the RL environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# RL Environment\n",
    "\n",
    "Now we compile these observations in a class to create all the functionality we require.\n",
    "\n",
    "## Expeience Replay (Replay Memory)\n",
    "- **sample_environment**: We sample random pairings to decouple bias, this is known as experience replay in the literature.\n",
    "- **next**: Sample the next $n$ observation in order.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Defining Computation\n",
    "\n",
    "RL Class should contain:\n",
    "- Environment\n",
    "- Model (given trained parameters)\n",
    "- Training Loop\n",
    "- Trainable Parameters\n",
    "- Graphics (training processes etc)\n",
    "\n",
    "There are two types of trainable model parameters:\n",
    "- Parameter values: torch.tensor()\n",
    "- Parameter distributions: numpy object()\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "We are working with a bandit as we only have $1$ state. We wish to maximise return by selecting the action that yields the highest expected return, that is, our policy $\\pi(\\alpha)$ is to select the action $\\alpha$ with the highest expected reward $Q_t(\\alpha)$ at time $t$:\n",
    "\n",
    "$$\\pi(\\alpha) = \\underset{\\alpha}{argmax} \\; Q_t(\\alpha)$$\n",
    "\n",
    "where the action space is given by the decision rule:\n",
    "\n",
    "$$actions = \\alpha = \\{\"shape\", \"colour\", \"number\" \\}$$\n",
    "\n",
    "----- \n",
    "The simplist calculation of this would be to let the expected return of each action be the sample average return from trying that action. That is to say:\n",
    "\n",
    "$$Q_t(\\alpha) = \\frac{1}{t-1}\\sum_1^{t-1}r_i$$\n",
    "\n",
    "where $r_t$ is the binary reward received after taking an action:\n",
    "\n",
    "$$r_t \\in\\{0,1\\}$$\n",
    "\n",
    "Note: for faster computation & more efficient memory, this average calculation can be written recursively:\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\frac{1}{n}[r_t - Q_{t-1}]$$\n",
    "\n",
    "Two problems arise:\n",
    "- This ignores non-stationarity (the rule changes) and as such more recent feedback must carry more weight.\n",
    "- This ignores the interaction effect of the game, knowledge about one action actually gives great info about the other actions as they are mutually exclusive. Should we encode this? It can be learnt through more aggressive updating.\n",
    "\n",
    "The $\\frac{1}{n}$ term effectively weights all returns $r_i$ equally. Thus this issues can be mitigated by overweighting newer returns & underweighting older returns. This can be achieved by setting this updating parameter to a constant $\\lambda$:\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\lambda[r_t - Q_{t-1}]$$\n",
    "\n",
    "-----\n",
    "\n",
    "Recall that $r_t \\in\\{0,1\\}$, thus:\n",
    "\n",
    "If $r_t = 1$ (correct choice):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= Q_{t-1} + \\lambda[r_t - Q_{t-1}] \\\\\n",
    "    Q_t &= Q_{t-1} + \\lambda[1 - Q_{t-1}] \\\\\n",
    "    Q_t &= \\lambda + (1-\\lambda)Q_{t-1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Or, if $r_t = 0$ (incorrect choice):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= Q_{t-1} + \\lambda[r_t - Q_{t-1}] \\\\\n",
    "    Q_t &= Q_{t-1} + \\lambda[0 - Q_{t-1}] \\\\\n",
    "    Q_t &= (1-\\lambda)Q_{t-1} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It's trivial to see that $\\lambda=1$ (the extreme case) would be choice the desired response:\n",
    "\n",
    "\n",
    "If $r_t = 1$ & $\\lambda=1$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= \\lambda + (1-\\lambda)Q_{t-1} \\\\\n",
    "    Q_t &= 1 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Or, if $r_t = 0$ & $\\lambda=1$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= (1-\\lambda)Q_{t-1} \\\\\n",
    "    Q_t &= 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which is correct as only one choice can be the correct choice. This is the extreme weighting, where we only take the value of the most recent return.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "# Resampling\n",
    "\n",
    "The agent should sample actions from a list & remove incorrect options, to ensure the agent cycles through all possible actions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-----\n",
    "\n",
    "# RL Instance\n",
    "On the implementation design:\n",
    "- Most memory + compute constraints arise from the Bayesian model, as such we wish to store lists of the Q-values etc so that we can monitor the algorithm overtime.\n",
    "- All $Q$ values are initialized at $0$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Next Steps\n",
    "\n",
    "How do we got from this baseline model to:\n",
    "- Incorporating experimental data \n",
    "- Capturing information from the other experiments\n",
    "- Capturing information from the other participants (Heirarchical Bayes)\n",
    "\n",
    "## Modeling Data\n",
    "\n",
    "Essentially fitting the this model to the data is equivalent to training the parameter $\\lambda$ - so that **the rate at which the action value approximations $Q_t(\\alpha)$ are updated - ie the HUMAN LEARNING RATE**\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\lambda[r_t - Q_{t-1}]$$\n",
    "\n",
    "#### RPE - Reward Prediction Error: $r_t - Q_{t-1}$\n",
    "\n",
    "All subsequent steps are to the same effect, to add information about this learnign rate:\n",
    "- Additional experiments can be caputured to add information about different learning rates between individuals\n",
    "- The Heirarchical Bayes can be used to capture variation (information) across individuals\n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "The model is too simple! \n",
    "\n",
    "_It is completely reasonable to assume that all participants will act with a learning rate of $\\lambda=1$ as if they understand the rules they will simply default to the last correct choice, i.e. $\\lambda=1$_\n",
    "\n",
    "As such:\n",
    "\n",
    "_*No additional statistical attributees will capture additonal information variation - are the additional experiments superflous?*_\n",
    "\n",
    "## MSc Project Solution + Additional paper\n",
    "\n",
    "Abstract the model design phase to building statistical tools/machinery to handle different types of psychological data.\n",
    "\n",
    "1. How to handle RL learning problems\n",
    "2. How to capture variation across sample (Bayes)\n",
    "3. Additional Statistical Machinery\n",
    "\n",
    "\n",
    "\n",
    "# Final Remarks\n",
    " - EEG data & follow on work? Can I assist early?\n",
    "    - Format of the data?\n",
    " - Phd Application in December: Happy to help on any applications/modeling/programming design tasks for coauthorship.\n",
    " - @Jonathan & @Allan to inform modeling direction.\n",
    "\n",
    "# Thoughts?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}