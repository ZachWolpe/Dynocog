{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd00bdb1145a6394107ddd55d824c4d0e411e79cd19a6286b1018600d724ae6ee81",
   "display_name": "Python 3.8.8 64-bit ('dynocog': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RL wcst Agent 001\n",
    "\n",
    "The first, simplest, RL agent. Showcasing the basic structure.\n",
    "\n",
    "## Optimality\n",
    "\n",
    "The optimal learning behaviour would be too simply choose the last correct action. This would equate to an update equation that totally saturates previous information & essentially only considers the most recent but of information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate dynocog\n",
    "# !conda init\n",
    "# !conda install pandas -y\n",
    "# !conda install -c pytorch pytorch -y\n",
    "# !conda install -c conda-forge numpyro\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hyperparameters ----x\n",
    "rules   = ['shape', 'number', 'color']\n",
    "colors  = ['yellow', 'red', 'green', 'blue']\n",
    "shapes  = ['star', 'triangle', 'circle', 'cross']\n",
    "numbers = [1,2,3,4]\n",
    "cards   = ['card 1', 'card 2', 'card 3', 'card 4']\n",
    "matching_cards = {\n",
    "  'card 1': {'color': 'red', 'shape':'circle', 'number':1},\n",
    "  'card 2': {'color': 'green', 'shape':'triangle', 'number':2},\n",
    "  'card 3': {'color': 'blue', 'shape':'cross', 'number':3}, \n",
    "  'card 4': {'color': 'yellow', 'shape':'star', 'number':4}}"
   ]
  },
  {
   "source": [
    "# Build RL Environment\n",
    "\n",
    "Now we build the RL environment that can be used to train the parameters.\n",
    "\n",
    "In our model the RL environment reflects WCST.\n",
    "\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "Dictionaries are far faster than lists in Python, as such we structure each sample as a dictionary. Each environmental sample has two components:\n",
    "- Current card: the card drawn\n",
    "- Target card: the correct matching option\n",
    "- Rule: the matching rule\n",
    "\n",
    "Each of these are captured in a dictionary & each observation put into a list object called 'item'. Each item is then added to a list (giving us a list of observations) which is later conververted into a enumerable object.\n",
    "\n",
    "Return: a list of observations & outputs representing the RL environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"../data/wcst.txt\", \"r\")\n",
    "\n",
    "all_items = []\n",
    "for l in f.readlines():\n",
    "    \n",
    "    # ---- target card ----x\n",
    "    ind = 'card ' + l[18]\n",
    "\n",
    "    # ---- current card ----x\n",
    "    curr_card = l[0:16].strip()\n",
    "    curr_card = re.split('([0-9]+)', curr_card)\n",
    "    curr_card = {'color': curr_card[2], 'shape':curr_card[0], 'number':curr_card[1]}\n",
    "\n",
    "    # ---- matching rule + target card ----x\n",
    "    rule = (l[27:27+8]).strip().replace('\\\"','')    \n",
    "    target_card = matching_cards[ind]\n",
    "\n",
    "    # ----- list current + target cards ----x\n",
    "    item = {'curr_card': curr_card, 'target_card':target_card, 'rule':rule}\n",
    "    all_items.append(item)"
   ]
  },
  {
   "source": [
    "# RL Environment\n",
    "\n",
    "Now we compile these observations in a class to create all the functionality we require.\n",
    "\n",
    "## Expeience Replay (Replay Memory)\n",
    "- **sample_environment**: We sample random pairings to decouple bias, this is known as experience replay in the literature.\n",
    "- **next**: Sample the next $n$ observation in order.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCST_Environment:\n",
    "    def __init__(self, all_items):\n",
    "        self.all_items = all_items\n",
    "        self.index     = 0\n",
    "        self.n         = len(all_items)\n",
    "        self.sample_idx = []\n",
    "        self.index = 0\n",
    "    \n",
    "    def sample_environment(self, n_samples):\n",
    "        idx = np.random.choice(range(0,self.n), n_samples)\n",
    "        self.sample_idx = self.sample_idx.append(idx)\n",
    "        return([self.all_items[i] for i in idx])\n",
    "    \n",
    "    def batch(self, n):\n",
    "        \"\"\"Return: n samples\n",
    "           Note:\n",
    "                - The function is cylical & will return the same sequence of samples periodically if a large batch is required.\n",
    "                - If one wishes to 'reset' & sample from the start either:\n",
    "                    1. set self.index == 0 & sample\n",
    "                    2. sample from self.all_items directly\n",
    "        \"\"\"\n",
    "        deck = []\n",
    "        nt = self.index + n\n",
    "        if nt > 100:    \n",
    "            loops = nt // 100 - 1    # integer devision == devision drop remainder\n",
    "                                    # number of full training loops\n",
    "            res   = nt % 100         # modulo == devision return remainder only\n",
    "                                    # number of items above 0\n",
    "            # --- until end ---x\n",
    "            for i in all_items[nt:]: deck.append(i)\n",
    "            # --- add 'loops' full data loops ----x\n",
    "            for i in range(loops):\n",
    "                for a in all_items: deck.append(a)\n",
    "            # --- remaining data ---x\n",
    "            for i in all_items[:res]: deck.append(i)\n",
    "            self.index = res\n",
    "\n",
    "        else: \n",
    "            deck = all_items[self.index:nt]\n",
    "            self.index = nt\n",
    "        return(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "source": [
    "wcst = WCST_Environment(all_items)\n",
    "batch = wcst.batch(7550)\n",
    "wcst.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7450"
      ]
     },
     "metadata": {},
     "execution_count": 331
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 275
    }
   ],
   "source": [
    "len(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Defining Computation\n",
    "\n",
    "RL Class should contain:\n",
    "- Environment\n",
    "- Model (given trained parameters)\n",
    "- Training Loop\n",
    "- Trainable Parameters\n",
    "- Graphics (training processes etc)\n",
    "\n",
    "There are two types of trainable model parameters:\n",
    "- Parameter values: torch.tensor()\n",
    "- Parameter distributions: numpy object()\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "We are working with a bandit as we only have $1$ state. We wish to maximise return by selecting the action that yields the highest expected return, that is, our policy $\\pi(\\alpha)$ is to select the action $\\alpha$ with the highest expected reward $Q_t(\\alpha)$ at time $t$:\n",
    "\n",
    "$$\\pi(\\alpha) = \\underset{\\alpha}{argmax} \\; Q_t(\\alpha)$$\n",
    "\n",
    "where the action space is given by the decision rule:\n",
    "\n",
    "$$actions = \\alpha = \\{\"shape\", \"colour\", \"number\" \\}$$\n",
    "\n",
    "----- \n",
    "The simplist calculation of this would be to let the expected return of each action be the sample average return from trying that action. That is to say:\n",
    "\n",
    "$$Q_t(\\alpha) = \\frac{1}{t-1}\\sum_1^{t-1}r_i$$\n",
    "\n",
    "where $r_t$ is the binary reward received after taking an action:\n",
    "\n",
    "$$r_t \\in\\{0,1\\}$$\n",
    "\n",
    "Note: for faster computation & more efficient memory, this average calculation can be written recursively:\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\frac{1}{n}[r_t - Q_{t-1}]$$\n",
    "\n",
    "Two problems arise:\n",
    "- This ignores non-stationarity (the rule changes) and as such more recent feedback must carry more weight.\n",
    "- This ignores the interaction effect of the game, knowledge about one action actually gives great info about the other actions as they are mutually exclusive. Should we encode this? It can be learnt through more aggressive updating.\n",
    "\n",
    "The $\\frac{1}{n}$ term effectively weights all returns $r_i$ equally. Thus this issues can be mitigated by overweighting newer returns & underweighting older returns. This can be achieved by setting this updating parameter to a constant $\\lambda$:\n",
    "\n",
    "$$Q_t = Q_{t-1} + \\lambda[r_t - Q_{t-1}]$$\n",
    "\n",
    "-----\n",
    "\n",
    "Recall that $r_t \\in\\{0,1\\}$, thus:\n",
    "\n",
    "If $r_t = 1$ (correct choice):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= Q_{t-1} + \\lambda[r_t - Q_{t-1}] \\\\\n",
    "    Q_t &= Q_{t-1} + \\lambda[1 - Q_{t-1}] \\\\\n",
    "    Q_t &= \\lambda + (1-\\lambda)Q_{t-1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Or, if $r_t = 0$ (incorrect choice):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= Q_{t-1} + \\lambda[r_t - Q_{t-1}] \\\\\n",
    "    Q_t &= Q_{t-1} + \\lambda[0 - Q_{t-1}] \\\\\n",
    "    Q_t &= (1-\\lambda)Q_{t-1} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It's trivial to see that $\\lambda=1$ (the extreme case) would be choice the desired response:\n",
    "\n",
    "\n",
    "If $r_t = 1$ & $\\lambda=1$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= \\lambda + (1-\\lambda)Q_{t-1} \\\\\n",
    "    Q_t &= 1 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Or, if $r_t = 0$ & $\\lambda=1$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Q_t &= (1-\\lambda)Q_{t-1} \\\\\n",
    "    Q_t &= 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which is correct as only one choice can be the correct choice. This is the extreme weighting, where we only take the value of the most recent return."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-----\n",
    "\n",
    "# RL Instance\n",
    "On the implementation design:\n",
    "- Most memory + compute constraints arise from the Bayesian model, as such we wish to store lists of the Q-values etc so that we can monitor the algorithm overtime.\n",
    "- All $Q$ values are initialized at $0$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcst = WCST_Environment(all_items)\n",
    "\n",
    "\n",
    "class RL_Instance:\n",
    "    \"\"\"Defining the RL model\"\"\"\n",
    "    def __init__(self):\n",
    "        self.actions  = ['colour', 'shape', 'number']\n",
    "        self.Q = {'colour': [0],\n",
    "                  'shape' : [0],\n",
    "                  'number': [0]}\n",
    "        self.a_t = [None]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- pass through the model ---x\n",
    "        Q_t = [self.Q['colour'][-1], self.Q['shape'][-1], self.Q['number'][-1]]\n",
    "        at  = self.actions[np.argmin(Q_t)]\n",
    "        self.a_t.append(at)\n",
    "        return(at)\n",
    "    \n",
    "\n",
    "\n",
    "    # REQUIREMENTS\n",
    "    # - Model (given trained parameters)\n",
    "    # - Training Loop\n",
    "    # - Trainable Parameters\n",
    "    # - Graphics (training processes etc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcst = WCST_Environment(all_items)\n",
    "x = wcst.sample_environment(2)\n",
    "Q = {'colour': [10],\n",
    "                  'shape' : [21],\n",
    "                  'number': [3]}\n",
    "actions  = ['colour', 'shape', 'number']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'number'"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "Q_t = [Q['colour'][-1], Q['shape'][-1], Q['number'][-1]]\n",
    "actions[np.argmin(Q_t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = RL_Instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'colour'"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "wcst = WCST_Environment(all_items)\n",
    "x = wcst.sample_environment(2)\n",
    "rl.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}